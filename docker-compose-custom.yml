version: '3'
services:

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: namenode
    volumes:
      - /tmp/hdfs/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop-hive.env
    ports:
      - "50070:50070"
      - "50470:50470"
      - "8020:8020"
      - "8022:8022"

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: datanode
    volumes:
      - /tmp/hdfs/datanode:/hadoop/dfs/data
      - ./bank:/bank
    env_file:
      - ./hadoop-hive.env
    environment:
      SERVICE_PRECONDITION: "namenode:50070"
    depends_on:
      - namenode
    ports:
      - "50075:50075"

  #hive-server:
  #  image: bde2020/hive:2.3.2-postgresql-metastore
  #  container_name: hive-server
  #  env_file:
  #    - ./hadoop-hive.env
  #  environment:
  #    HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
  #    SERVICE_PRECONDITION: "hive-metastore:9083"
  #  ports:
  #    - "10000:10000"
  #  depends_on:
  #    - hive-metastore

  #hive-metastore:
  #  image: bde2020/hive:2.3.2-postgresql-metastore
  #  container_name: hive-metastore
  #  env_file:
  #    - ./hadoop-hive.env
  #  command: /opt/hive/bin/hive --service metastore
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:50070 datanode:50075 hive-metastore-postgresql:5432"
  #  ports:
  #    - "9083:9083"
  #  depends_on:
  #    - hive-metastore-postgresql

  #hive-metastore-postgresql:
  #  image: bde2020/hive-metastore-postgresql:2.3.0
  #  container_name: hive-metastore-postgresql
  #  depends_on:
  #    - datanode

  hue:
      image: gethue/hue:20191107-135001
      hostname: hue
      container_name: hue
      dns: 8.8.8.8
      ports:
      - "8888:8888"
      volumes:
        - ./hue-overrides.ini:/usr/share/hue/desktop/conf/z-hue.ini
      depends_on:
      - "database"

  database:
      image: mysql:5.7
      container_name: database
      ports:
          - "33061:3306"
      command: --init-file /data/application/init.sql
      volumes:
          - /tmp/mysql/data:/var/lib/mysql
          - ./init.sql:/data/application/init.sql
      environment:
          MYSQL_ROOT_USER: root
          MYSQL_ROOT_PASSWORD: secret
          MYSQL_DATABASE: hue
          MYSQL_USER: root
          MYSQL_PASSWORD: secret

  zookeeper:
    image: confluentinc/cp-zookeeper:5.3.1
    container_name: zookeeper
    hostname: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:5.3.1
    container_name: broker
    hostname: broker
    ports:
      - "9092:9092"
      - "19092:19092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENERS: 'PLAINTEXT://:9092,CONNECTIONS_FROM_HOST://:19092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker:9092,CONNECTIONS_FROM_HOST://localhost:19092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

  schema-registry:
    image: confluentinc/cp-schema-registry:5.3.1
    container_name: schema-registry
    hostname: schema-registry
    depends_on:
      - broker
      - zookeeper
    ports:
      - "8181:8181"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: 'schema-registry'
      SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8181'
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'PLAINTEXT://broker:9092'
      SCHEMA_REGISTRY_SCHEMA_REGISTRY_INTER_INSTANCE_PROTOCOL: http
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _schemas

  kafka-connect:
    image: confluentinc/cp-kafka-connect:5.3.1
    container_name: kafka-connect
    hostname: kafka-connect
    depends_on:
      - zookeeper
      - broker
      - schema-registry
    ports:
      - "8083:8083"
      - "6007:6007"
    volumes:
      #- ../kafka-connect-hdfs-10.0.0/target/kafka-connect-hdfs-10.0.0-package/share/java/kafka-connect-hdfs:/etc/kafka-connect/jars/kafka-connect-hdfs:rw
      - ../kafka-connect-hdfs-multi-schema-10.0.0/target/kafka-connect-hdfs-multi-schema-10.0.0-development/share/java/kafka-connect-multi-schema-hdfs:/etc/kafka-connect/jars/kafka-connect-multi-schema-hdfs:rw
      - ../kafka-connect-hdfs-10.0.0/target/kafka-connect-hdfs-10.0.0-development/share/java/kafka-connect-hdfs:/etc/kafka-connect/jars/kafka-connect-hdfs:rw
      - ./hdfs-site.xml:/etc/kafka-connect/hdfs-site.xml
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:9092'
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "hub-notification"
      CONNECT_CONFIG_STORAGE_TOPIC: "hub-config"
      CONNECT_OFFSET_STORAGE_TOPIC: "hub-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "hub-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8181"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8181"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: 'org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR'
      CONNECT_PLUGIN_PATH: /usr/share/java,/etc/kafka-connect/jars
      KAFKA_HEAP_OPTS: "-Xms256M -Xmx1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Dhttp.nonProxyHosts=\"*.portoseguro.com.br\""
      EXTRA_ARGS: "-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=6007"
      CONNECT_CONNECT_PROTOCOL: 'compatible'

  akhq:
    image: tchiotludo/akhq:dev
    container_name: akhq
    ports:
      - "9090:8080"
    volumes:
      - ./application.yml:/app/application.yml

    #networks:
    #  net_pet:
    #    ipv4_address: 172.27.1.20

  #streamsets:
  #  image: streamsets/datacollector:3.13.0-latest
  #  ports:
  #    - "18630:18630"
  #  networks:
  #    net_pet:
  #      ipv4_address: 172.27.1.17

#networks:
#  net_pet:
#    ipam:
#      driver: default
#      config:
#        - subnet: 172.27.0.0/16

===========
<?xml version="1.0" encoding="UTF-8"?>

<configuration>
  <property>
    <name>dfs.nameservices</name>
    <value>nameservice1</value>
  </property>
  <property>
    <name>dfs.client.failover.proxy.provider.nameservice1</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.automatic-failover.enabled.nameservice1</name>
    <value>true</value>
  </property>
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>zookeeper:2181</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.nameservice1</name>
    <value>namenode383</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.nameservice1.namenode383</name>
    <value>namenode:8020</value>
  </property>
  <property>
    <name>dfs.namenode.servicerpc-address.nameservice1.namenode383</name>
    <value>namenode:8022</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.nameservice1.namenode383</name>
    <value>namenode:50070</value>
  </property>
  <property>
    <name>dfs.namenode.https-address.nameservice1.namenode383</name>
    <value>namenode:50470</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.blocksize</name>
    <value>268435456</value>
  </property>
  <property>
    <name>dfs.client.use.datanode.hostname</name>
    <value>false</value>
  </property>
  <property>
    <name>fs.permissions.umask-mode</name>
    <value>006</value>
  </property>
  <property>
    <name>dfs.namenode.acls.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.client.use.legacy.blockreader</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.domain.socket.path</name>
    <value>/var/run/hdfs-sockets/dn</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit.skip.checksum</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.client.domain.socket.data.traffic</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.block.access.token.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.namenode.kerberos.principal</name>
    <value>hdfs/_HOST@HADOOP1.CORP1.RC.ITAU</value>
  </property>
  <property>
    <name>dfs.namenode.kerberos.internal.spnego.principal</name>
    <value>HTTP/_HOST@HADOOP1.CORP1.RC.ITAU</value>
  </property>
  <property>
    <name>dfs.datanode.kerberos.principal</name>
    <value>hdfs/_HOST@HADOOP1.CORP1.RC.ITAU</value>
  </property>
  <property>
    <name>dfs.client.block.write.retries</name>
    <value>6</value>
  </property>
  <property>
    <name>dfs.client.block.write.locateFollowingBlock.retries</name>
    <value>10</value>
  </property>
  <property>
    <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
    <value>ALWAYS</value>
  </property>
  <property>
    <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name>
    <value>TRUE</value>
  </property>
</configuration>

===========


===========
